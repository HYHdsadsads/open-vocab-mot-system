"""è·¨æ¨¡æ€è¯å…¸æ¨¡å—ï¼ˆå®Œæ•´å®ç°ç‰ˆï¼‰"""
import cv2
import numpy as np
import torch
from sklearn.cluster import KMeans
from scipy.spatial import ConvexHull
from PIL import Image
import torch.nn.functional as F
from collections import deque, Counter
import clip
from config import ZERO_SHOT_CONFIG, DICTIONARY_CONFIG, MODEL_CONFIG
from typing import List, Tuple, Dict, Any, Optional


class CrossModalDictionary:
    def __init__(self, clip_model, clip_preprocess, yolo_model, device="cpu", gpt_api_key=None):
        self.device = device or MODEL_CONFIG["device"]

        # è§†è§‰æŠ•å½±ç½‘ç»œï¼šå°†åŸå§‹è§†è§‰ç‰¹å¾æŠ•å½±åˆ°ä¸è¯­è¨€ç‰¹å¾å¯¹é½çš„ç©ºé—´
        self.visual_projection = torch.nn.Linear(512, 512).to(self.device)
        # åˆå§‹åŒ–æŠ•å½±ç½‘ç»œæƒé‡ï¼ˆæå‡è®­ç»ƒç¨³å®šæ€§ï¼‰
        torch.nn.init.xavier_uniform_(self.visual_projection.weight)
        torch.nn.init.zeros_(self.visual_projection.bias)

        # åŠ¨æ€æƒé‡é¢„æµ‹ç½‘ç»œï¼šæ ¹æ®æ¨¡æ€å¯é æ€§å’Œç‰¹å¾ç›¸ä¼¼åº¦é¢„æµ‹èåˆæƒé‡
        self.weight_predictor = torch.nn.Sequential(
            torch.nn.Linear(512 * 2 + 2, 256),  # æ–°å¢æ¨¡æ€å¯é æ€§è¾“å…¥ï¼ˆ2ç»´ï¼‰
            torch.nn.LayerNorm(256),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.1),
            torch.nn.Linear(256, 2),
            torch.nn.Softmax(dim=-1)
        ).to(self.device)

        # ä¼˜åŒ–å™¨ï¼šåˆå¹¶æŠ•å½±ç½‘ç»œå’Œæƒé‡é¢„æµ‹ç½‘ç»œå‚æ•°
        self.optimizer = torch.optim.Adam(
            list(self.visual_projection.parameters()) +
            list(self.weight_predictor.parameters()),
            lr=DICTIONARY_CONFIG.get("lr", 1e-4),
            weight_decay=1e-5  # å¢åŠ æƒé‡è¡°å‡é˜²æ­¢è¿‡æ‹Ÿåˆ
        )

        # æ¨¡å‹ä¸é¢„å¤„ç†å·¥å…·
        self.clip_model = clip_model
        self.clip_preprocess = clip_preprocess
        self.yolo_model = yolo_model

        # æ•°æ®å­˜å‚¨ç»“æ„
        self.language_atoms = {}          # è¯­è¨€åŸå­ç‰¹å¾ {class_name: np.ndarray(512,)}
        self.visual_atoms = {}            # è§†è§‰åŸå­ç‰¹å¾ {class_name: List[np.ndarray(512,)]}
        self.combined_atoms = {}          # èåˆåŸå­ç‰¹å¾ {class_name: np.ndarray(512,)}
        self.historical_atoms = {}        # å†å²èåˆåŸå­ {class_name: deque[np.ndarray(512,)]}
        self.feature_weights = {}         # ç‰¹å¾èåˆæƒé‡ {class_name: np.ndarray(2,)} 0:è§†è§‰æƒé‡, 1:è¯­è¨€æƒé‡
        self.classes = []                 # å·²çŸ¥ç±»åˆ«åˆ—è¡¨

        # GPTç›¸å…³é…ç½®ï¼ˆè¯­ä¹‰å¢å¼ºï¼‰
        self.gpt_api_key = gpt_api_key
        self.gpt_semantic_generator = None

        # éš¾æ ·æœ¬æŒ–æ˜å‚æ•°ï¼ˆå¯é€šè¿‡é…ç½®æ–‡ä»¶è°ƒæ•´ï¼‰
        self.hard_neg_threshold = DICTIONARY_CONFIG.get("hard_neg_threshold", 0.7)
        self.hard_pos_threshold = DICTIONARY_CONFIG.get("hard_pos_threshold", 0.3)

        # æ—¶é—´æ¼”åŒ–å‚æ•°ï¼ˆä¿æŒç‰¹å¾ç¨³å®šæ€§ï¼‰
        self.atom_evolution_window = ZERO_SHOT_CONFIG.get("atom_evolution_window", 10)
        self.temporal_consistency_weight = ZERO_SHOT_CONFIG.get("temporal_consistency_weight", 0.2)

        # èšç±»å‚æ•°ï¼ˆè§†è§‰åŸå­æ„å»ºï¼‰
        self.num_clusters = DICTIONARY_CONFIG.get("num_clusters", 5)  # æ¯ä¸ªç±»åˆ«èšç±»æ•°é‡

        # åˆå§‹åŒ–GPTå®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼‰
        if gpt_api_key:
            try:
                import openai
                openai.api_key = gpt_api_key
                self.gpt_semantic_generator = openai
            except ImportError:
                print("âš ï¸ è¯·å®‰è£…openaiåº“ä»¥ä½¿ç”¨GPTè¯­ä¹‰å¢å¼ºåŠŸèƒ½: pip install openai")
            except Exception as e:
                print(f"âš ï¸ GPTå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")

    def triplet_loss_with_hard_mining(self, anchor: torch.Tensor, positive: torch.Tensor, negative: torch.Tensor) -> torch.Tensor:
        """
        æ”¹è¿›çš„ä¸‰å…ƒç»„æŸå¤±ï¼ˆéš¾æ ·æœ¬æŒ–æ˜ï¼‰
        ç›®æ ‡ï¼šæ‹‰è¿‘åŒç±»åˆ«ï¼ˆanchor-positiveï¼‰è·ç¦»ï¼Œæ‹‰è¿œä¸åŒç±»åˆ«ï¼ˆanchor-negativeï¼‰è·ç¦»
        """
        # è®¡ç®—æ¬§æ°è·ç¦»
        pos_dist = F.pairwise_distance(anchor, positive, p=2)  # æ­£æ ·æœ¬è·ç¦» [batch_size,]
        neg_dist = F.pairwise_distance(anchor, negative, p=2)  # è´Ÿæ ·æœ¬è·ç¦» [batch_size,]

        # éš¾æ­£æ ·æœ¬ï¼šè·ç¦»å¤§äºé˜ˆå€¼çš„æ­£æ ·æœ¬å¯¹ï¼ˆéš¾åŒºåˆ†çš„åŒç±»åˆ«ï¼‰
        hard_pos_mask = pos_dist > self.hard_pos_threshold
        # éš¾è´Ÿæ ·æœ¬ï¼šè·ç¦»å°äºé˜ˆå€¼çš„è´Ÿæ ·æœ¬å¯¹ï¼ˆéš¾åŒºåˆ†çš„ä¸åŒç±»åˆ«ï¼‰
        hard_neg_mask = neg_dist < self.hard_neg_threshold

        # è¿‡æ»¤æœ‰æ•ˆéš¾æ ·æœ¬
        hard_pos_dist = pos_dist[hard_pos_mask]
        hard_neg_dist = neg_dist[hard_neg_mask]

        # ç¡®ä¿éš¾æ ·æœ¬æ•°é‡åŒ¹é…
        min_hard_count = min(len(hard_pos_dist), len(hard_neg_dist))
        if min_hard_count > 0:
            # åªä½¿ç”¨åŒ¹é…æ•°é‡çš„éš¾æ ·æœ¬è®¡ç®—æŸå¤±
            hard_pos_dist = hard_pos_dist[:min_hard_count]
            hard_neg_dist = hard_neg_dist[:min_hard_count]
            # ä¸‰å…ƒç»„æŸå¤±ï¼špos_dist - neg_dist + margin > 0 æ—¶äº§ç”ŸæŸå¤±
            loss = F.relu(hard_pos_dist - hard_neg_dist + DICTIONARY_CONFIG["triplet_loss_margin"]).mean()
        else:
            # æ— éš¾æ ·æœ¬æ—¶ï¼Œä½¿ç”¨æ™®é€šä¸‰å…ƒç»„æŸå¤±
            loss = F.relu(pos_dist - neg_dist + DICTIONARY_CONFIG["triplet_loss_margin"]).mean()

        return loss

    def _get_current_bbox(self, detection: Tuple[List[float], np.ndarray, str]) -> List[float]:
        """ä»æ£€æµ‹ç»“æœä¸­å®‰å…¨è·å–è¾¹ç•Œæ¡†"""
        if not detection or len(detection) < 1:
            return [0.0, 0.0, 0.0, 0.0]
        bbox = detection[0]
        # ç¡®ä¿è¾¹ç•Œæ¡†æ ¼å¼æ­£ç¡®ï¼ˆx1,y1,x2,y2ï¼‰
        if len(bbox) != 4:
            return [0.0, 0.0, 0.0, 0.0]
        return [float(x) for x in bbox]

    def compute_iou(self, bbox1: List[float], bbox2: List[float]) -> float:
        """è®¡ç®—ä¸¤ä¸ªè¾¹ç•Œæ¡†çš„äº¤å¹¶æ¯”ï¼ˆIOUï¼‰"""
        x1, y1, x2, y2 = bbox1
        x3, y3, x4, y4 = bbox2

        # è®¡ç®—äº¤é›†åŒºåŸŸ
        inter_x1 = max(x1, x3)
        inter_y1 = max(y1, y3)
        inter_x2 = min(x2, x4)
        inter_y2 = min(y2, y4)

        # äº¤é›†é¢ç§¯ï¼ˆé˜²æ­¢è´Ÿé¢ç§¯ï¼‰
        inter_area = max(0.0, inter_x2 - inter_x1) * max(0.0, inter_y2 - inter_y1)
        # ä¸¤ä¸ªè¾¹ç•Œæ¡†é¢ç§¯
        area1 = (x2 - x1) * (y2 - y1)
        area2 = (x4 - x3) * (y4 - y3)
        # å¹¶é›†é¢ç§¯ï¼ˆé˜²æ­¢é™¤ä»¥0ï¼‰
        union_area = max(1e-8, area1 + area2 - inter_area)

        return inter_area / union_area

    def get_class_frequency(self) -> Counter:
        """è·å–æ¯ä¸ªç±»åˆ«çš„è§†è§‰ç‰¹å¾å‡ºç°é¢‘ç‡ï¼ˆç”¨äºè¯­è¨€å¯é æ€§è¯„ä¼°ï¼‰"""
        freq = Counter()
        for cls_name, feat_list in self.visual_atoms.items():
            freq[cls_name] = len(feat_list)  # ç‰¹å¾æ•°é‡å³å‡ºç°é¢‘ç‡
        return freq

    def compute_modal_reliability(self,
                                 visual_feat: torch.Tensor,
                                 lang_feat: torch.Tensor,
                                 image: Optional[np.ndarray] = None,
                                 detection: Optional[Tuple[List[float], np.ndarray, str]] = None,
                                 class_name: Optional[str] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å¤šç»´åº¦æ¨¡æ€å¯é æ€§è¯„ä¼°
        è¿”å›ï¼š(è§†è§‰å¯é æ€§, è¯­è¨€å¯é æ€§)ï¼Œå€¼åŸŸ [0.2, 1.0]
        """
        # -------------------------- 1. è§†è§‰å¯é æ€§è¯„ä¼° --------------------------
        # èåˆï¼šæ¸…æ™°åº¦ï¼ˆæ‹‰æ™®æ‹‰æ–¯æ–¹å·®ï¼‰+ é®æŒ¡ç¨‹åº¦ + å…‰ç…§å‡åŒ€åº¦
        visual_reliability = torch.tensor(0.8, device=self.device, dtype=torch.float32)  # é»˜è®¤å€¼

        if image is not None and detection is not None:
            try:
                # 1.1 æ¸…æ™°åº¦è¯„ä¼°ï¼ˆæ‹‰æ™®æ‹‰æ–¯æ–¹å·®è¶Šå¤§è¶Šæ¸…æ™°ï¼‰
                gray_img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
                laplacian_var = cv2.Laplacian(gray_img, cv2.CV_64F).var()
                sharpness = np.clip(laplacian_var / 300.0, 0.2, 1.0)  # 300ä¸ºç»éªŒé˜ˆå€¼

                # 1.2 é®æŒ¡ç¨‹åº¦è¯„ä¼°ï¼ˆæ£€æµ‹æ¡†ä¸å›¾åƒè¾¹ç•Œçš„äº¤å ï¼‰
                h, w = image.shape[:2]
                bbox = self._get_current_bbox(detection)
                full_img_bbox = [0.0, 0.0, float(w), float(h)]
                iou_with_full = self.compute_iou(bbox, full_img_bbox)
                occlusion_factor = np.clip(iou_with_full, 0.2, 1.0)  # IOUè¶Šå°é®æŒ¡è¶Šä¸¥é‡

                # 1.3 å…‰ç…§å‡åŒ€åº¦è¯„ä¼°ï¼ˆHSVç©ºé—´äº®åº¦æ ‡å‡†å·®ï¼‰
                hsv_img = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
                brightness_std = np.std(hsv_img[:, :, 2])  # äº®åº¦é€šé“æ ‡å‡†å·®
                light_factor = np.clip(1 - brightness_std / 50.0, 0.2, 1.0)  # æ ‡å‡†å·®è¶Šå°è¶Šå‡åŒ€

                # åŠ æƒèåˆè§†è§‰å¯é æ€§
                visual_reliability = torch.tensor(
                    0.5 * sharpness + 0.3 * occlusion_factor + 0.2 * light_factor,
                    device=self.device, dtype=torch.float32
                )
            except Exception as e:
                print(f"âš ï¸ è§†è§‰å¯é æ€§è®¡ç®—å¤±è´¥: {e}")

        # -------------------------- 2. è¯­è¨€å¯é æ€§è¯„ä¼° --------------------------
        # èåˆï¼šç±»åˆ«é¢‘ç‡ + è¯­ä¹‰æ­§ä¹‰åº¦
        lang_reliability = torch.tensor(0.7, device=self.device, dtype=torch.float32)  # é»˜è®¤å€¼

        if class_name:
            try:
                # 2.1 ç±»åˆ«é¢‘ç‡å› å­ï¼ˆé¢‘ç‡è¶Šä½ï¼Œè¯­è¨€ç‰¹å¾è¶Šä¸å¯é ï¼‰
                class_freq = self.get_class_frequency()
                freq = class_freq.get(class_name, 1)
                freq_factor = np.clip(1 / (np.log(freq + 2)), 0.3, 1.0)  # å¯¹æ•°å¹³æ»‘

                # 2.2 è¯­ä¹‰æ­§ä¹‰åº¦å› å­ï¼ˆåŸºäºGPTç”Ÿæˆæè¿°çš„ç†µï¼‰
                if self.gpt_semantic_generator:
                    ambiguity = self._compute_semantic_ambiguity(class_name)
                    ambiguity_factor = np.clip(1 - ambiguity, 0.3, 1.0)  # ç†µè¶Šå°æ­§ä¹‰åº¦è¶Šä½
                else:
                    ambiguity_factor = 0.6  # æ— GPTæ—¶ä½¿ç”¨ç»éªŒå€¼

                # åŠ æƒèåˆè¯­è¨€å¯é æ€§
                lang_reliability = torch.tensor(
                    0.6 * freq_factor + 0.4 * ambiguity_factor,
                    device=self.device, dtype=torch.float32
                )
            except Exception as e:
                print(f"âš ï¸ è¯­è¨€å¯é æ€§è®¡ç®—å¤±è´¥: {e}")

        return visual_reliability, lang_reliability

    @staticmethod
    def cluster_visual_atoms(features: List[np.ndarray], n_clusters: int = 5) -> Optional[KMeans]:
        """
        é™æ€æ–¹æ³•ï¼šå¯¹è§†è§‰ç‰¹å¾è¿›è¡ŒKMeansèšç±»ï¼Œæå–ä»£è¡¨æ€§è§†è§‰åŸå­
        è¾“å…¥ï¼šç‰¹å¾åˆ—è¡¨ [N, 512]
        è¾“å‡ºï¼šKMeansèšç±»æ¨¡å‹ï¼ˆå«èšç±»ä¸­å¿ƒï¼‰
        """
        # 1. è¾“å…¥éªŒè¯
        if not features:
            print("âŒ èšç±»å¤±è´¥ï¼šæ— è¾“å…¥ç‰¹å¾")
            return None

        # 2. ç»Ÿä¸€ç‰¹å¾ç»´åº¦å¹¶è¿‡æ»¤æ— æ•ˆç‰¹å¾
        feature_dim = 512
        valid_features = []
        for feat in features:
            # è¿‡æ»¤ç»´åº¦ä¸ç¬¦çš„ç‰¹å¾
            if len(feat) != feature_dim:
                print(f"âš ï¸ è¿‡æ»¤æ— æ•ˆç‰¹å¾ï¼ˆç»´åº¦ {len(feat)}ï¼ŒæœŸæœ› {feature_dim}ï¼‰")
                continue
            # è¿‡æ»¤å…¨é›¶ç‰¹å¾
            if np.allclose(feat, 0):
                print("âš ï¸ è¿‡æ»¤å…¨é›¶ç‰¹å¾")
                continue
            valid_features.append(feat)

        # 3. ç¡®ä¿æœ‰è¶³å¤Ÿçš„æœ‰æ•ˆç‰¹å¾
        if len(valid_features) < 2:
            print(f"âŒ èšç±»å¤±è´¥ï¼šæœ‰æ•ˆç‰¹å¾ä»… {len(valid_features)} ä¸ªï¼ˆéœ€è‡³å°‘2ä¸ªï¼‰")
            return None

        # 4. è°ƒæ•´èšç±»æ•°é‡ï¼ˆä¸è¶…è¿‡æœ‰æ•ˆç‰¹å¾æ•°ï¼‰
        n_clusters = min(n_clusters, len(valid_features))
        if n_clusters < 1:
            n_clusters = 1

        # 5. æ‰§è¡ŒKMeansèšç±»ï¼ˆå¢åŠ å¼‚å¸¸æ•è·ï¼‰
        try:
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)  # n_init=10æå‡ç¨³å®šæ€§
            kmeans.fit(np.array(valid_features))
            print(f"âœ… èšç±»å®Œæˆï¼š{len(valid_features)} ä¸ªç‰¹å¾èšä¸º {n_clusters} ç±»")
            return kmeans
        except Exception as e:
            print(f"âŒ èšç±»æ‰§è¡Œå¤±è´¥ï¼š{str(e)}")
            return None

    def _compute_semantic_ambiguity(self, class_name: str) -> float:
        """
        åŸºäºGPTç”Ÿæˆæè¿°çš„ç†µè®¡ç®—è¯­ä¹‰æ­§ä¹‰åº¦
        ç†µè¶Šå¤§ï¼Œè¯­ä¹‰è¶Šæ¨¡ç³Šï¼›å€¼åŸŸ [0, 1]
        """
        try:
            # ç”Ÿæˆ3æ¬¡ä¸åŒçš„è§†è§‰ç‰¹å¾æè¿°ï¼ˆå¢åŠ éšæœºæ€§ï¼‰
            responses = []
            prompts = [
                f"ç”¨3ä¸ªå…³é”®è¯æè¿° {class_name} çš„æ ¸å¿ƒè§†è§‰ç‰¹å¾",
                f"ç®€è¦è¯´æ˜ {class_name} çš„å½¢çŠ¶ã€é¢œè‰²ã€ç»“æ„ç­‰è§†è§‰å±æ€§ï¼ˆ3ä¸ªå…³é”®è¯ï¼‰",
                f"{class_name} çš„ç‹¬ç‰¹è§†è§‰æ ‡è¯†æ˜¯ä»€ä¹ˆï¼Ÿç”¨3ä¸ªè¯å›ç­”"
            ]

            for prompt in prompts:
                response = self.gpt_semantic_generator.ChatCompletion.create(
                    model="gpt-3.5-turbo",
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.7  # æ§åˆ¶éšæœºæ€§
                )
                responses.append(response.choices[0].message.content.strip().lower())

            # æ„å»ºè¯è¢‹å¹¶è®¡ç®—ç†µ
            words = []
            for resp in responses:
                # æå–å…³é”®è¯ï¼ˆè¿‡æ»¤æ ‡ç‚¹å’Œæ— æ„ä¹‰è¯ï¼‰
                valid_words = [w for w in resp.split() if len(w) > 1 and w not in ["å’Œ", "çš„", "æ˜¯", "æœ‰"]]
                words.extend(valid_words)

            if not words:
                print(f"âš ï¸ {class_name} æ— æœ‰æ•ˆè¯­ä¹‰å…³é”®è¯")
                return 0.5

            # è®¡ç®—è¯é¢‘åˆ†å¸ƒçš„ç†µ
            word_counts = Counter(words)
            total_words = len(words)
            probs = [count / total_words for count in word_counts.values()]
            entropy = -sum(p * np.log(p + 1e-8) for p in probs)  # é˜²æ­¢log(0)
            normalized_entropy = entropy / np.log(len(words) + 1e-8)  # å½’ä¸€åŒ–åˆ° [0, 1]

            print(f"ğŸ“Š {class_name} è¯­ä¹‰æ­§ä¹‰åº¦ï¼š{normalized_entropy:.3f}")
            return np.clip(normalized_entropy, 0.0, 1.0)
        except Exception as e:
            print(f"âš ï¸ è¯­ä¹‰æ­§ä¹‰åº¦è®¡ç®—å¤±è´¥ï¼š{e}")
            return 0.5  # å‡ºé”™æ—¶è¿”å›ä¸­é—´å€¼

    def dynamic_feature_fusion(self,
                              visual_feat: torch.Tensor,
                              lang_feat: torch.Tensor,
                              image: Optional[np.ndarray] = None,
                              detection: Optional[Tuple[List[float], np.ndarray, str]] = None,
                              class_name: Optional[str] = None) -> torch.Tensor:
        """
        åŠ¨æ€ç‰¹å¾èåˆï¼ˆæ ¸å¿ƒåŠŸèƒ½ï¼‰
        ç»“åˆï¼š1. ç‰¹å¾ç›¸ä¼¼åº¦ 2. æ¨¡æ€å¯é æ€§ 3. åŠ¨æ€æƒé‡é¢„æµ‹
        è¾“å…¥ï¼š
            visual_feat: è§†è§‰ç‰¹å¾ [512,] æˆ– [batch_size, 512]
            lang_feat: è¯­è¨€ç‰¹å¾ [512,] æˆ– [batch_size, 512]
            image: åŸå§‹å›¾åƒï¼ˆç”¨äºå¯é æ€§è¯„ä¼°ï¼‰
            detection: æ£€æµ‹ç»“æœï¼ˆbbox, feat, class_nameï¼‰
            class_name: ç›®æ ‡ç±»åˆ«åç§°
        è¾“å‡ºï¼šèåˆç‰¹å¾ [512,] æˆ– [batch_size, 512]
        """
        # 1. ç»Ÿä¸€è¾“å…¥ç»´åº¦ï¼ˆç¡®ä¿ä¸º2Då¼ é‡ [batch_size, 512]ï¼‰
        is_single = False
        if visual_feat.dim() == 1:
            visual_feat = visual_feat.unsqueeze(0)
            lang_feat = lang_feat.unsqueeze(0)
            is_single = True

        # 2. è®¡ç®—æ¨¡æ€å¯é æ€§ï¼ˆè§†è§‰+è¯­è¨€ï¼‰
        vis_rel, lang_rel = self.compute_modal_reliability(
            visual_feat, lang_feat, image, detection, class_name
        )
        # æ‰©å±•ä¸ºbatchç»´åº¦
        vis_rel = vis_rel.unsqueeze(0).repeat(visual_feat.shape[0], 1)  # [batch_size, 1]
        lang_rel = lang_rel.unsqueeze(0).repeat(visual_feat.shape[0], 1)  # [batch_size, 1]

        # 3. è®¡ç®—ç‰¹å¾ç›¸ä¼¼åº¦ï¼ˆè§†è§‰ä¸è¯­è¨€ç‰¹å¾çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        feat_sim = F.cosine_similarity(visual_feat, lang_feat, dim=-1).unsqueeze(1)  # [batch_size, 1]
        feat_sim = torch.sigmoid(feat_sim)  # å½’ä¸€åŒ–åˆ° [0, 1]

        # 4. åŠ¨æ€æƒé‡é¢„æµ‹ï¼ˆè¾“å…¥ï¼šç‰¹å¾æ‹¼æ¥ + å¯é æ€§ + ç›¸ä¼¼åº¦ï¼‰
        cat_feat = torch.cat([
            visual_feat, lang_feat,  # ç‰¹å¾ [batch_size, 1024]
            vis_rel, lang_rel,       # å¯é æ€§ [batch_size, 2]
            feat_sim                 # ç›¸ä¼¼åº¦ [batch_size, 1]
        ], dim=-1)  # [batch_size, 1024+2+1=1027]

        raw_weights = self.weight_predictor(cat_feat)  # [batch_size, 2] åŸå§‹æƒé‡

        # 5. æƒé‡è°ƒæ•´ï¼ˆç»“åˆæ¨¡æ€å¯é æ€§ï¼‰
        adjusted_vis_weight = raw_weights[:, 0:1] * vis_rel  # è§†è§‰æƒé‡ = é¢„æµ‹æƒé‡ * è§†è§‰å¯é æ€§
        adjusted_lang_weight = raw_weights[:, 1:2] * lang_rel  # è¯­è¨€æƒé‡ = é¢„æµ‹æƒé‡ * è¯­è¨€å¯é æ€§

        # 6. æƒé‡å½’ä¸€åŒ–ï¼ˆç¡®ä¿å’Œä¸º1ï¼‰
        total_weight = adjusted_vis_weight + adjusted_lang_weight + 1e-8
        normalized_vis_weight = adjusted_vis_weight / total_weight
        normalized_lang_weight = adjusted_lang_weight / total_weight

        # 7. ç‰¹å¾èåˆï¼ˆåŠ æƒæ±‚å’Œï¼‰
        fused_feat = normalized_vis_weight * visual_feat + normalized_lang_weight * lang_feat
        # ç‰¹å¾å½’ä¸€åŒ–ï¼ˆæå‡åç»­è®¡ç®—ç¨³å®šæ€§ï¼‰
        fused_feat = F.normalize(fused_feat, dim=-1, p=2)

        # 8. æ¢å¤åŸå§‹ç»´åº¦ï¼ˆå•æ ·æœ¬æ—¶è¿”å›1Då¼ é‡ï¼‰
        if is_single:
            fused_feat = fused_feat.squeeze(0)

        # è®°å½•èåˆæƒé‡ï¼ˆå•æ ·æœ¬æ—¶ï¼‰
        if is_single and class_name:
            self.feature_weights[class_name] = np.array([
                normalized_vis_weight.item(),
                normalized_lang_weight.item()
            ])

        return fused_feat

    def generate_language_embedding_with_gpt(self, class_name: str) -> Optional[np.ndarray]:
        """
        ä½¿ç”¨GPTç”Ÿæˆå¢å¼ºçš„è¯­è¨€åµŒå…¥ï¼ˆæ¯”åŸºç¡€CLIPæ–‡æœ¬ç‰¹å¾æ›´ç²¾å‡†ï¼‰
        è¾“å‡ºï¼š512ç»´è¯­è¨€ç‰¹å¾
        """
        if not self.gpt_semantic_generator:
            print(f"âš ï¸ æ— GPT APIå¯†é’¥ï¼Œä½¿ç”¨åŸºç¡€CLIPæ–‡æœ¬ç‰¹å¾ for {class_name}")
            return None

        try:
            # ç”Ÿæˆè¯¦ç»†çš„è§†è§‰æè¿°
            response = self.gpt_semantic_generator.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯è®¡ç®—æœºè§†è§‰ä¸“å®¶ï¼Œéœ€è¦ç”Ÿæˆç›®æ ‡çš„è¯¦ç»†è§†è§‰æè¿°ï¼Œç”¨äºç‰¹å¾æå–"},
                    {"role": "user", "content": f"è¯¦ç»†æè¿° {class_name} çš„è§†è§‰ç‰¹å¾ï¼ŒåŒ…æ‹¬ï¼šå½¢çŠ¶ã€é¢œè‰²ã€å¤§å°æ¯”ä¾‹ã€è¡¨é¢çº¹ç†ã€å…¸å‹å§¿æ€/çŠ¶æ€ã€ä¸å…¶ä»–ç›¸ä¼¼ç›®æ ‡çš„åŒºåˆ«ã€‚ç”¨3-5å¥è¯æè¿°ï¼Œçªå‡ºå¯åŒºåˆ†çš„è§†è§‰å±æ€§ã€‚"}
                ],
                temperature=0.6
            )
            detailed_desc = response.choices[0].message.content.strip()
            print(f"ğŸ“ {class_name} GPTæè¿°ï¼š{detailed_desc}")

            # ç¼–ç ä¸ºCLIPç‰¹å¾
            text = clip.tokenize([detailed_desc]).to(self.device)
            with torch.no_grad():
                text_feat = self.clip_model.encode_text(text)
                text_feat = F.normalize(text_feat, dim=-1)  # å½’ä¸€åŒ–

            return text_feat.cpu().numpy().flatten()
        except Exception as e:
            print(f"âš ï¸ GPTè¯­è¨€ç‰¹å¾ç”Ÿæˆå¤±è´¥ for {class_name}: {e}")
            return None

    def align_features_with_triplet(self,
                                   visual_feats: List[np.ndarray],
                                   text_feats: List[np.ndarray],
                                   class_labels: List[str]) -> float:
        """
        ä½¿ç”¨ä¸‰å…ƒç»„æŸå¤±å¯¹é½è§†è§‰å’Œè¯­è¨€ç‰¹å¾ï¼ˆè®­ç»ƒæ ¸å¿ƒï¼‰
        è¾“å…¥ï¼š
            visual_feats: è§†è§‰ç‰¹å¾åˆ—è¡¨ [N, 512]
            text_feats: è¯­è¨€ç‰¹å¾åˆ—è¡¨ [N, 512]
            class_labels: ç±»åˆ«æ ‡ç­¾åˆ—è¡¨ [N,]
        è¾“å‡ºï¼šæŸå¤±å€¼
        """
        # æ ·æœ¬æ•°é‡æ£€æŸ¥ï¼ˆå¤ªå°‘åˆ™ä¸è®­ç»ƒï¼‰
        if len(visual_feats) < 32 or len(text_feats) < 32 or len(class_labels) < 32:
            print(f"âš ï¸ æ ·æœ¬ä¸è¶³ï¼Œè·³è¿‡ç‰¹å¾å¯¹é½ï¼ˆéœ€è‡³å°‘32ä¸ªæ ·æœ¬ï¼Œå½“å‰ {len(visual_feats)} ä¸ªï¼‰")
            return 0.0

        # è½¬æ¢ä¸ºå¼ é‡å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
        visual_tensor = torch.tensor(np.array(visual_feats), dtype=torch.float32).to(self.device)
        text_tensor = torch.tensor(np.array(text_feats), dtype=torch.float32).to(self.device)

        # è§†è§‰ç‰¹å¾æŠ•å½±ï¼ˆå¯¹é½åˆ°è¯­è¨€ç‰¹å¾ç©ºé—´ï¼‰
        projected_visual = self.visual_projection(visual_tensor)
        projected_visual = F.normalize(projected_visual, dim=-1)  # å½’ä¸€åŒ–

        # é‡‡æ ·é”šç‚¹ã€æ­£æ ·æœ¬ã€è´Ÿæ ·æœ¬ï¼ˆæ‰¹é‡é‡‡æ ·128ä¸ªï¼‰
        batch_size = min(128, len(visual_feats))
        anchor_indices = torch.randint(0, len(visual_feats), (batch_size,), device=self.device)
        anchors = projected_visual[anchor_indices]  # [batch_size, 512]

        # æ­£æ ·æœ¬ï¼šä¸é”šç‚¹åŒç±»åˆ«ä¸”è·ç¦»æœ€è¿œçš„æ ·æœ¬ï¼ˆéš¾æ­£æ ·æœ¬ï¼‰
        positives = []
        for idx in anchor_indices:
            cls = class_labels[idx]
            # åŒç±»åˆ«æ ·æœ¬ç´¢å¼•
            same_cls_indices = [i for i, label in enumerate(class_labels) if label == cls and i != idx]
            if not same_cls_indices:
                # æ— åŒç±»åˆ«æ ·æœ¬æ—¶ï¼Œä½¿ç”¨é”šç‚¹è‡ªèº«
                positives.append(anchors[len(positives)])
                continue
            # é€‰æ‹©è·ç¦»æœ€è¿œçš„åŒç±»åˆ«æ ·æœ¬ï¼ˆéš¾æ­£æ ·æœ¬ï¼‰
            same_cls_feats = projected_visual[same_cls_indices]
            dists = F.pairwise_distance(anchors[len(positives):len(positives)+1], same_cls_feats)
            hard_pos_idx = same_cls_indices[torch.argmax(dists)]
            positives.append(projected_visual[hard_pos_idx])
        positives = torch.stack(positives)  # [batch_size, 512]

        # è´Ÿæ ·æœ¬ï¼šä¸é”šç‚¹ä¸åŒç±»åˆ«ä¸”è·ç¦»æœ€è¿‘çš„æ ·æœ¬ï¼ˆéš¾è´Ÿæ ·æœ¬ï¼‰
        negatives = []
        for idx in anchor_indices:
            cls = class_labels[idx]
            # ä¸åŒç±»åˆ«æ ·æœ¬ç´¢å¼•
            diff_cls_indices = [i for i, label in enumerate(class_labels) if label != cls]
            if not diff_cls_indices:
                # æ— ä¸åŒç±»åˆ«æ ·æœ¬æ—¶ï¼Œä½¿ç”¨éšæœºç‰¹å¾
                negatives.append(torch.randn_like(anchors[0], device=self.device))
                continue
            # é€‰æ‹©è·ç¦»æœ€è¿‘çš„ä¸åŒç±»åˆ«æ ·æœ¬ï¼ˆéš¾è´Ÿæ ·æœ¬ï¼‰
            diff_cls_feats = projected_visual[diff_cls_indices]
            dists = F.pairwise_distance(anchors[len(negatives):len(negatives)+1], diff_cls_feats)
            hard_neg_idx = diff_cls_indices[torch.argmin(dists)]
            negatives.append(projected_visual[hard_neg_idx])
        negatives = torch.stack(negatives)  # [batch_size, 512]

        # è®¡ç®—ä¸‰å…ƒç»„æŸå¤±å¹¶åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss = self.triplet_loss_with_hard_mining(anchors, positives, negatives)
        loss.backward()
        self.optimizer.step()  # æ›´æ–°æ¨¡å‹å‚æ•°

        return loss.item()

    def initialize_from_classes(self, class_names: List[str]):
        """
        ä»ç±»åˆ«åˆ—è¡¨åˆå§‹åŒ–è·¨æ¨¡æ€è¯å…¸
        1. ç”Ÿæˆè¯­è¨€åŸå­ç‰¹å¾ï¼ˆä¼˜å…ˆGPTå¢å¼ºï¼Œ fallbackåˆ°åŸºç¡€CLIPï¼‰
        2. åˆå§‹åŒ–æ•°æ®ç»“æ„
        """
        if not class_names:
            raise ValueError("âŒ ç±»åˆ«åˆ—è¡¨ä¸èƒ½ä¸ºç©º")

        self.classes = class_names
        # åˆå§‹åŒ–å†å²åŸå­é˜Ÿåˆ—ï¼ˆé™åˆ¶çª—å£å¤§å°ï¼‰
        self.historical_atoms = {cls: deque(maxlen=self.atom_evolution_window) for cls in class_names}
        # åˆå§‹åŒ–è§†è§‰åŸå­å­˜å‚¨
        self.visual_atoms = {cls: [] for cls in class_names}

        print(f"ğŸš€ åˆå§‹åŒ–è·¨æ¨¡æ€è¯å…¸ï¼Œç±»åˆ«æ•°ï¼š{len(class_names)}")
        for cls in class_names:
            # ä¼˜å…ˆä½¿ç”¨GPTç”Ÿæˆå¢å¼ºè¯­è¨€ç‰¹å¾
            gpt_lang_feat = self.generate_language_embedding_with_gpt(cls)
            if gpt_lang_feat is not None and len(gpt_lang_feat) == 512:
                self.language_atoms[cls] = gpt_lang_feat
                continue

            # Fallbackï¼šä½¿ç”¨åŸºç¡€CLIPæ–‡æœ¬ç‰¹å¾
            try:
                text = clip.tokenize([f"a photo of a {cls}"]).to(self.device)
                with torch.no_grad():
                    clip_lang_feat = self.clip_model.encode_text(text)
                    clip_lang_feat = F.normalize(clip_lang_feat, dim=-1)
                self.language_atoms[cls] = clip_lang_feat.cpu().numpy().flatten()
                print(f"âœ… {cls} åŸºç¡€CLIPè¯­è¨€ç‰¹å¾åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                print(f"âŒ {cls} è¯­è¨€ç‰¹å¾åˆå§‹åŒ–å¤±è´¥: {e}")
                self.language_atoms[cls] = np.random.randn(512)  # å¼‚å¸¸æ—¶ä½¿ç”¨éšæœºç‰¹å¾

    def extract_visual_features(self, image: np.ndarray, bbox: List[float]) -> Optional[np.ndarray]:
        """
        æå–å•ä¸ªç›®æ ‡çš„è§†è§‰ç‰¹å¾
        è¾“å…¥ï¼šåŸå§‹å›¾åƒ + ç›®æ ‡è¾¹ç•Œæ¡†ï¼ˆx1,y1,x2,y2ï¼‰
        è¾“å‡ºï¼š512ç»´CLIPè§†è§‰ç‰¹å¾
        """
        # è¾¹ç•Œæ¡†æœ‰æ•ˆæ€§æ£€æŸ¥
        x1, y1, x2, y2 = map(int, bbox)
        h, w = image.shape[:2]
        x1 = max(0, x1)
        y1 = max(0, y1)
        x2 = min(w, x2)
        y2 = min(h, y2)

        # æå–ç›®æ ‡ROIï¼ˆæ„Ÿå…´è¶£åŒºåŸŸï¼‰
        roi = image[y1:y2, x1:x2]
        if roi.size == 0:
            print("âš ï¸ ç›®æ ‡ROIä¸ºç©ºï¼Œè·³è¿‡ç‰¹å¾æå–")
            return None

        # è½¬æ¢ä¸ºPILå›¾åƒå¹¶åº”ç”¨CLIPé¢„å¤„ç†
        try:
            pil_roi = Image.fromarray(cv2.cvtColor(roi, cv2.COLOR_BGR2RGB))
            preprocessed = self.clip_preprocess(pil_roi).unsqueeze(0).to(self.device)

            # æå–CLIPè§†è§‰ç‰¹å¾
            with torch.no_grad():
                visual_feat = self.clip_model.encode_image(preprocessed)
                visual_feat = F.normalize(visual_feat, dim=-1)

            return visual_feat.cpu().numpy().flatten()
        except Exception as e:
            print(f"âš ï¸ å•ä¸ªè§†è§‰ç‰¹å¾æå–å¤±è´¥: {e}")
            return None

    def extract_visual_features_batch(self, image: np.ndarray, bboxes: List[List[float]]) -> List[np.ndarray]:
        """
        æ‰¹é‡æå–ç›®æ ‡è§†è§‰ç‰¹å¾ï¼ˆæå‡æ•ˆç‡ï¼‰
        è¾“å…¥ï¼šåŸå§‹å›¾åƒ + è¾¹ç•Œæ¡†åˆ—è¡¨
        è¾“å‡ºï¼šè§†è§‰ç‰¹å¾åˆ—è¡¨ [N, 512]
        """
        if not bboxes:
            return []

        preprocessed_imgs = []
        valid_indices = []  # è®°å½•æœ‰æ•ˆè¾¹ç•Œæ¡†ç´¢å¼•

        for idx, bbox in enumerate(bboxes):
            # è¾¹ç•Œæ¡†æœ‰æ•ˆæ€§æ£€æŸ¥
            x1, y1, x2, y2 = map(int, bbox)
            h, w = image.shape[:2]
            x1 = max(0, x1)
            y1 = max(0, y1)
            x2 = min(w, x2)
            y2 = min(h, y2)

            # æå–ROI
            roi = image[y1:y2, x1:x2]
            if roi.size == 0:
                print(f"âš ï¸ è¾¹ç•Œæ¡† {idx} ROIä¸ºç©ºï¼Œè·³è¿‡")
                continue

            # é¢„å¤„ç†å¹¶ä¿å­˜
            try:
                pil_roi = Image.fromarray(cv2.cvtColor(roi, cv2.COLOR_BGR2RGB))
                preprocessed = self.clip_preprocess(pil_roi)
                preprocessed_imgs.append(preprocessed)
                valid_indices.append(idx)
            except Exception as e:
                print(f"âš ï¸ è¾¹ç•Œæ¡† {idx} é¢„å¤„ç†å¤±è´¥: {e}")
                continue

        # æ‰¹é‡æå–ç‰¹å¾
        if not preprocessed_imgs:
            return []

        try:
            batch_imgs = torch.stack(preprocessed_imgs).to(self.device)
            with torch.no_grad():
                batch_feats = self.clip_model.encode_image(batch_imgs)
                batch_feats = F.normalize(batch_feats, dim=-1)
            # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶è¿”å›
            return [feat.cpu().numpy().flatten() for feat in batch_feats]
        except Exception as e:
            print(f"âš ï¸ æ‰¹é‡è§†è§‰ç‰¹å¾æå–å¤±è´¥: {e}")
            return []

    def update_visual_atoms(self, class_name: str, visual_feat: np.ndarray):
        """
        æ›´æ–°æŒ‡å®šç±»åˆ«çš„è§†è§‰åŸå­ç‰¹å¾
        1. è¿‡æ»¤æ— æ•ˆç‰¹å¾
        2. ç´¯è®¡ç‰¹å¾åèšç±»æå–ä»£è¡¨æ€§åŸå­
        """
        if class_name not in self.classes:
            print(f"âš ï¸ ç±»åˆ« {class_name} ä¸åœ¨è¯å…¸ä¸­ï¼Œè·³è¿‡è§†è§‰åŸå­æ›´æ–°")
            return

        # è¿‡æ»¤æ— æ•ˆç‰¹å¾
        if len(visual_feat) != 512 or np.allclose(visual_feat, 0):
            print(f"âš ï¸ æ— æ•ˆè§†è§‰ç‰¹å¾ï¼Œè·³è¿‡ {class_name} åŸå­æ›´æ–°")
            return

        # æ·»åŠ åˆ°è§†è§‰ç‰¹å¾åˆ—è¡¨
        self.visual_atoms[class_name].append(visual_feat)
        print(f"ğŸ“¥ {class_name} ç´¯è®¡è§†è§‰ç‰¹å¾æ•°ï¼š{len(self.visual_atoms[class_name])}")

        # å½“ç‰¹å¾æ•°è¾¾åˆ°é˜ˆå€¼æ—¶ï¼Œèšç±»æ›´æ–°è§†è§‰åŸå­
        cluster_threshold = DICTIONARY_CONFIG.get("cluster_threshold", 20)  # èšç±»é˜ˆå€¼ï¼ˆ20ä¸ªç‰¹å¾ï¼‰
        if len(self.visual_atoms[class_name]) >= cluster_threshold:
            # æ‰§è¡Œèšç±»
            kmeans = self.cluster_visual_atoms(
                self.visual_atoms[class_name],
                n_clusters=self.num_clusters
            )
            if kmeans is not None:
                # èšç±»ä¸­å¿ƒä½œä¸ºæ–°çš„è§†è§‰åŸå­
                self.visual_atoms[class_name] = kmeans.cluster_centers_.tolist()
                print(f"âœ… {class_name} è§†è§‰åŸå­æ›´æ–°å®Œæˆï¼ˆ{self.num_clusters} ä¸ªèšç±»ä¸­å¿ƒï¼‰")
            else:
                # èšç±»å¤±è´¥æ—¶ï¼Œä¿ç•™æœ€è¿‘20ä¸ªç‰¹å¾
                self.visual_atoms[class_name] = self.visual_atoms[class_name][-20:]

    def align_and_combine(self):
        """
        æ ¸å¿ƒæµç¨‹ï¼šç‰¹å¾å¯¹é½ + åŠ¨æ€èåˆ + æ—¶é—´æ¼”åŒ–
        1. ä½¿ç”¨ä¸‰å…ƒç»„æŸå¤±å¯¹é½è§†è§‰å’Œè¯­è¨€ç‰¹å¾
        2. åŠ¨æ€èåˆç”Ÿæˆè·¨æ¨¡æ€åŸå­
        3. ç»“åˆå†å²åŸå­ä¿æŒæ—¶é—´ä¸€è‡´æ€§
        """
        print("ğŸ”„ å¼€å§‹ç‰¹å¾å¯¹é½ä¸èåˆ...")
        for class_name in self.classes:
            # æ£€æŸ¥å¿…è¦æ•°æ®æ˜¯å¦å­˜åœ¨
            if class_name not in self.language_atoms or len(self.visual_atoms[class_name]) == 0:
                print(f"âš ï¸ {class_name} æ•°æ®ä¸å®Œæ•´ï¼Œè·³è¿‡èåˆ")
                continue

            # å‡†å¤‡å¯¹é½æ•°æ®
            visual_feats = self.visual_atoms[class_name]  # [M, 512]
            lang_feat = self.language_atoms[class_name]    # [512,]
            # æ‰©å±•è¯­è¨€ç‰¹å¾åˆ°æ‰¹é‡ç»´åº¦
            text_feats = [lang_feat for _ in visual_feats]  # [M, 512]
            class_labels = [class_name for _ in visual_feats]  # [M,]

            # 1. ç‰¹å¾å¯¹é½ï¼ˆè®­ç»ƒæŠ•å½±ç½‘ç»œå’Œæƒé‡é¢„æµ‹ç½‘ç»œï¼‰
            alignment_loss = self.align_features_with_triplet(visual_feats, text_feats, class_labels)
            print(f"ğŸ“Š {class_name} ç‰¹å¾å¯¹é½æŸå¤±ï¼š{alignment_loss:.4f}")

            # 2. åŠ¨æ€èåˆï¼ˆä½¿ç”¨å¯¹é½åçš„ç‰¹å¾ï¼‰
            with torch.no_grad():
                # æŠ•å½±è§†è§‰ç‰¹å¾
                visual_tensor = torch.tensor(np.array(visual_feats), dtype=torch.float32).to(self.device)
                projected_visual = self.visual_projection(visual_tensor)
                projected_visual = F.normalize(projected_visual, dim=-1)

                # è¯­è¨€ç‰¹å¾è½¬æ¢ä¸ºå¼ é‡
                lang_tensor = torch.tensor(lang_feat, dtype=torch.float32).to(self.device)

                # æ‰¹é‡åŠ¨æ€èåˆ
                fused_feats = []
                for vis_feat in projected_visual:
                    fused_feat = self.dynamic_feature_fusion(
                        vis_feat, lang_tensor,
                        class_name=class_name
                    )
                    fused_feats.append(fused_feat.cpu().numpy())

                # èåˆç‰¹å¾å‡å€¼ï¼ˆä»£è¡¨è¯¥ç±»åˆ«çš„è·¨æ¨¡æ€åŸå­ï¼‰
                current_fused_atom = np.mean(fused_feats, axis=0)
                current_fused_atom = current_fused_atom / np.linalg.norm(current_fused_atom)  # å½’ä¸€åŒ–

            # 3. æ—¶é—´æ¼”åŒ–ï¼ˆç»“åˆå†å²åŸå­ä¿æŒç¨³å®šæ€§ï¼‰
            if class_name in self.historical_atoms and len(self.historical_atoms[class_name]) > 0:
                # å†å²åŸå­å‡å€¼
                historical_mean = np.mean(list(self.historical_atoms[class_name]), axis=0)
                historical_mean = historical_mean / np.linalg.norm(historical_mean)
                # åŠ æƒèåˆå½“å‰åŸå­å’Œå†å²åŸå­
                current_fused_atom = (1 - self.temporal_consistency_weight) * current_fused_atom + \
                                     self.temporal_consistency_weight * historical_mean
                # é‡æ–°å½’ä¸€åŒ–
                current_fused_atom = current_fused_atom / np.linalg.norm(current_fused_atom)

            # 4. æ›´æ–°è¯å…¸
            self.combined_atoms[class_name] = current_fused_atom
            self.historical_atoms[class_name].append(current_fused_atom)
            print(f"âœ… {class_name} è·¨æ¨¡æ€åŸå­æ›´æ–°å®Œæˆ")

    def get_atom(self, class_name: str) -> Optional[np.ndarray]:
        """
        è·å–æŒ‡å®šç±»åˆ«çš„è·¨æ¨¡æ€åŸå­ç‰¹å¾
        ä¼˜å…ˆçº§ï¼šèåˆåŸå­ > è¯­è¨€åŸå­ > è§†è§‰åŸå­å‡å€¼
        """
        # 1. ä¼˜å…ˆè¿”å›èåˆåŸå­ï¼ˆæœ€ä¼˜ï¼‰
        if class_name in self.combined_atoms:
            return self.combined_atoms[class_name].copy()
        # 2. å…¶æ¬¡è¿”å›è¯­è¨€åŸå­
        elif class_name in self.language_atoms:
            return self.language_atoms[class_name].copy()
        # 3. æœ€åè¿”å›è§†è§‰åŸå­å‡å€¼
        elif class_name in self.visual_atoms and len(self.visual_atoms[class_name]) > 0:
            visual_mean = np.mean(self.visual_atoms[class_name], axis=0)
            return visual_mean / np.linalg.norm(visual_mean)
        else:
            print(f"âŒ æœªæ‰¾åˆ° {class_name} çš„åŸå­ç‰¹å¾")
            return None
"""
é¢å‘è¾¹ç¼˜è®¡ç®—çš„å¼‚æ„æ¨ç†åŠ é€Ÿä¸é‡åŒ–ç ”ç©¶
åŒ…å«ï¼šç®—å­èåˆåˆ†æã€é‡åŒ–ç­–ç•¥ã€æ¨ç†å»¶è¿Ÿåˆ†è§£
"""
import torch
import torch.nn as nn
import numpy as np
import time
from typing import Dict, List, Tuple, Optional
import json
from collections import defaultdict


class OperatorFusionAnalyzer:
    """ç®—å­èåˆåˆ†æå™¨"""
    
    def __init__(self, model: nn.Module):
        self.model = model
        self.fusion_stats = defaultdict(int)
        self.memory_savings = {}
        
    def analyze_conv_bn_relu_fusion(self) -> Dict:
        """åˆ†æ Conv+BN+ReLU èåˆ"""
        fusion_opportunities = []
        total_params = 0
        fused_params = 0
        
        modules = list(self.model.named_modules())
        
        for i in range(len(modules) - 2):
            name1, module1 = modules[i]
            name2, module2 = modules[i + 1]
            name3, module3 = modules[i + 2]
            
            # æ£€æµ‹ Conv -> BN -> ReLU æ¨¡å¼
            if (isinstance(module1, nn.Conv2d) and 
                isinstance(module2, nn.BatchNorm2d) and
                isinstance(module3, nn.ReLU)):
                
                # è®¡ç®—å‚æ•°é‡
                conv_params = sum(p.numel() for p in module1.parameters())
                bn_params = sum(p.numel() for p in module2.parameters())
                
                fusion_opportunities.append({
                    "pattern": "Conv+BN+ReLU",
                    "layers": [name1, name2, name3],
                    "conv_params": conv_params,
                    "bn_params": bn_params,
                    "can_fuse": True
                })
                
                total_params += conv_params + bn_params
                fused_params += conv_params  # èåˆååªä¿ç•™ Conv å‚æ•°
                
                self.fusion_stats["Conv+BN+ReLU"] += 1
        
        # è®¡ç®—å†…å­˜èŠ‚çœ
        memory_saved = (total_params - fused_params) * 4 / (1024 * 1024)  # MB
        
        return {
            "fusion_opportunities": fusion_opportunities,
            "total_fusions": len(fusion_opportunities),
            "memory_saved_mb": memory_saved,
            "fusion_stats": dict(self.fusion_stats)
        }
    
    def analyze_linear_fusion(self) -> Dict:
        """åˆ†æ Linear å±‚èåˆ"""
        fusion_opportunities = []
        
        modules = list(self.model.named_modules())
        
        for i in range(len(modules) - 1):
            name1, module1 = modules[i]
            name2, module2 = modules[i + 1]
            
            # æ£€æµ‹ Linear -> ReLU/GELU æ¨¡å¼
            if isinstance(module1, nn.Linear):
                if isinstance(module2, (nn.ReLU, nn.GELU)):
                    fusion_opportunities.append({
                        "pattern": f"Linear+{module2.__class__.__name__}",
                        "layers": [name1, name2],
                        "can_fuse": True
                    })
                    self.fusion_stats[f"Linear+{module2.__class__.__name__}"] += 1
        
        return {
            "fusion_opportunities": fusion_opportunities,
            "total_fusions": len(fusion_opportunities),
            "fusion_stats": dict(self.fusion_stats)
        }
    
    def generate_fusion_report(self) -> Dict:
        """ç”Ÿæˆå®Œæ•´èåˆæŠ¥å‘Š"""
        conv_fusion = self.analyze_conv_bn_relu_fusion()
        linear_fusion = self.analyze_linear_fusion()
        
        total_fusions = conv_fusion["total_fusions"] + linear_fusion["total_fusions"]
        
        return {
            "conv_bn_relu_fusion": conv_fusion,
            "linear_fusion": linear_fusion,
            "total_fusion_opportunities": total_fusions,
            "estimated_speedup": 1.0 + (total_fusions * 0.05),  # æ¯ä¸ªèåˆçº¦ 5% åŠ é€Ÿ
            "memory_saved_mb": conv_fusion.get("memory_saved_mb", 0)
        }


class QuantizationStrategy:
    """é‡åŒ–ç­–ç•¥é€‰æ‹©å™¨"""
    
    def __init__(self, model: nn.Module, device="cpu"):
        self.model = model
        self.device = device
        self.quantization_results = {}
        
    def benchmark_fp32(self, dummy_input: torch.Tensor, num_runs: int = 100) -> Dict:
        """FP32 åŸºå‡†æµ‹è¯•"""
        self.model.eval()
        self.model.to(self.device)
        dummy_input = dummy_input.to(self.device)
        
        # é¢„çƒ­
        with torch.no_grad():
            for _ in range(10):
                _ = self.model(dummy_input)
        
        # æµ‹è¯•
        latencies = []
        with torch.no_grad():
            for _ in range(num_runs):
                start = time.time()
                _ = self.model(dummy_input)
                if self.device == "cuda":
                    torch.cuda.synchronize()
                latencies.append((time.time() - start) * 1000)  # ms
        
        return {
            "precision": "FP32",
            "avg_latency_ms": np.mean(latencies),
            "std_latency_ms": np.std(latencies),
            "min_latency_ms": np.min(latencies),
            "max_latency_ms": np.max(latencies),
            "model_size_mb": sum(p.numel() * 4 for p in self.model.parameters()) / (1024 * 1024)
        }
    
    def benchmark_fp16(self, dummy_input: torch.Tensor, num_runs: int = 100) -> Dict:
        """FP16 åŸºå‡†æµ‹è¯•"""
        if self.device != "cuda":
            return {"error": "FP16 requires CUDA"}
        
        model_fp16 = self.model.half()
        dummy_input_fp16 = dummy_input.half().to(self.device)
        
        # é¢„çƒ­
        with torch.no_grad():
            for _ in range(10):
                _ = model_fp16(dummy_input_fp16)
        
        # æµ‹è¯•
        latencies = []
        with torch.no_grad():
            for _ in range(num_runs):
                start = time.time()
                _ = model_fp16(dummy_input_fp16)
                torch.cuda.synchronize()
                latencies.append((time.time() - start) * 1000)
        
        return {
            "precision": "FP16",
            "avg_latency_ms": np.mean(latencies),
            "std_latency_ms": np.std(latencies),
            "min_latency_ms": np.min(latencies),
            "max_latency_ms": np.max(latencies),
            "model_size_mb": sum(p.numel() * 2 for p in self.model.parameters()) / (1024 * 1024)
        }
    
    def benchmark_int8(self, dummy_input: torch.Tensor, num_runs: int = 100) -> Dict:
        """INT8 é‡åŒ–åŸºå‡†æµ‹è¯•"""
        try:
            # åŠ¨æ€é‡åŒ–
            model_int8 = torch.quantization.quantize_dynamic(
                self.model,
                {nn.Linear, nn.Conv2d},
                dtype=torch.qint8
            )
            
            dummy_input = dummy_input.to(self.device)
            
            # é¢„çƒ­
            with torch.no_grad():
                for _ in range(10):
                    _ = model_int8(dummy_input)
            
            # æµ‹è¯•
            latencies = []
            with torch.no_grad():
                for _ in range(num_runs):
                    start = time.time()
                    _ = model_int8(dummy_input)
                    if self.device == "cuda":
                        torch.cuda.synchronize()
                    latencies.append((time.time() - start) * 1000)
            
            # ä¼°ç®—æ¨¡å‹å¤§å°
            model_size = sum(p.numel() for p in model_int8.parameters()) / (1024 * 1024)
            
            return {
                "precision": "INT8",
                "avg_latency_ms": np.mean(latencies),
                "std_latency_ms": np.std(latencies),
                "min_latency_ms": np.min(latencies),
                "max_latency_ms": np.max(latencies),
                "model_size_mb": model_size
            }
        except Exception as e:
            return {"error": f"INT8 quantization failed: {str(e)}"}
    
    def compare_all_precisions(self, dummy_input: torch.Tensor) -> Dict:
        """å¯¹æ¯”æ‰€æœ‰ç²¾åº¦"""
        results = {}
        
        print("ğŸ” æµ‹è¯• FP32...")
        results["fp32"] = self.benchmark_fp32(dummy_input)
        
        if self.device == "cuda":
            print("ğŸ” æµ‹è¯• FP16...")
            results["fp16"] = self.benchmark_fp16(dummy_input)
        
        print("ğŸ” æµ‹è¯• INT8...")
        results["int8"] = self.benchmark_int8(dummy_input)
        
        # è®¡ç®—åŠ é€Ÿæ¯”
        fp32_latency = results["fp32"]["avg_latency_ms"]
        
        if "fp16" in results and "avg_latency_ms" in results["fp16"]:
            results["fp16"]["speedup"] = fp32_latency / results["fp16"]["avg_latency_ms"]
        
        if "avg_latency_ms" in results["int8"]:
            results["int8"]["speedup"] = fp32_latency / results["int8"]["avg_latency_ms"]
        
        return results


class InferenceProfiler:
    """æ¨ç†å»¶è¿Ÿåˆ†è§£åˆ†æå™¨"""
    
    def __init__(self, model: nn.Module, device="cpu"):
        self.model = model
        self.device = device
        self.layer_times = {}
        
    def profile_layers(self, dummy_input: torch.Tensor, num_runs: int = 50) -> Dict:
        """é€å±‚æ€§èƒ½åˆ†æ"""
        self.model.eval()
        self.model.to(self.device)
        dummy_input = dummy_input.to(self.device)
        
        layer_times = defaultdict(list)
        
        # æ³¨å†Œé’©å­
        hooks = []
        
        def make_hook(name):
            def hook(module, input, output):
                start = time.time()
                # æ¨¡æ‹Ÿå±‚æ‰§è¡Œæ—¶é—´
                if self.device == "cuda":
                    torch.cuda.synchronize()
                elapsed = (time.time() - start) * 1000
                layer_times[name].append(elapsed)
            return hook
        
        # ä¸ºæ¯ä¸€å±‚æ³¨å†Œé’©å­
        for name, module in self.model.named_modules():
            if len(list(module.children())) == 0:  # å¶å­èŠ‚ç‚¹
                hooks.append(module.register_forward_hook(make_hook(name)))
        
        # è¿è¡Œæ¨ç†
        with torch.no_grad():
            for _ in range(num_runs):
                _ = self.model(dummy_input)
        
        # ç§»é™¤é’©å­
        for hook in hooks:
            hook.remove()
        
        # ç»Ÿè®¡ç»“æœ
        layer_stats = {}
        total_time = 0
        
        for name, times in layer_times.items():
            avg_time = np.mean(times)
            layer_stats[name] = {
                "avg_time_ms": avg_time,
                "std_time_ms": np.std(times),
                "percentage": 0  # ç¨åè®¡ç®—
            }
            total_time += avg_time
        
        # è®¡ç®—ç™¾åˆ†æ¯”
        for name in layer_stats:
            layer_stats[name]["percentage"] = (layer_stats[name]["avg_time_ms"] / total_time) * 100
        
        return {
            "layer_stats": layer_stats,
            "total_time_ms": total_time,
            "num_layers": len(layer_stats)
        }
    
    def generate_breakdown_chart_data(self, profile_results: Dict) -> Dict:
        """ç”Ÿæˆå»¶è¿Ÿåˆ†è§£é¥¼å›¾æ•°æ®"""
        layer_stats = profile_results["layer_stats"]
        
        # æŒ‰å±‚ç±»å‹åˆ†ç»„
        type_times = defaultdict(float)
        
        for name, stats in layer_stats.items():
            # è¯†åˆ«å±‚ç±»å‹
            if "conv" in name.lower():
                layer_type = "Convolution"
            elif "linear" in name.lower() or "fc" in name.lower():
                layer_type = "Linear"
            elif "bn" in name.lower() or "batch" in name.lower():
                layer_type = "BatchNorm"
            elif "relu" in name.lower() or "gelu" in name.lower():
                layer_type = "Activation"
            elif "pool" in name.lower():
                layer_type = "Pooling"
            elif "attention" in name.lower():
                layer_type = "Attention"
            else:
                layer_type = "Other"
            
            type_times[layer_type] += stats["avg_time_ms"]
        
        total_time = sum(type_times.values())
        
        # ç”Ÿæˆé¥¼å›¾æ•°æ®
        chart_data = {
            "labels": list(type_times.keys()),
            "values": list(type_times.values()),
            "percentages": [(v / total_time) * 100 for v in type_times.values()]
        }
        
        return chart_data


class EdgeOptimizationPipeline:
    """è¾¹ç¼˜è®¡ç®—ä¼˜åŒ–å®Œæ•´æµç¨‹"""
    
    def __init__(self, model: nn.Module, device="cpu"):
        self.model = model
        self.device = device
        
        self.fusion_analyzer = OperatorFusionAnalyzer(model)
        self.quantization_strategy = QuantizationStrategy(model, device)
        self.profiler = InferenceProfiler(model, device)
        
    def run_full_optimization_analysis(self, dummy_input: torch.Tensor) -> Dict:
        """è¿è¡Œå®Œæ•´ä¼˜åŒ–åˆ†æ"""
        print("=" * 60)
        print("ğŸš€ è¾¹ç¼˜è®¡ç®—ä¼˜åŒ–åˆ†æå¼€å§‹")
        print("=" * 60)
        
        results = {}
        
        # 1. ç®—å­èåˆåˆ†æ
        print("\nğŸ“Š 1. ç®—å­èåˆåˆ†æ...")
        results["fusion_analysis"] = self.fusion_analyzer.generate_fusion_report()
        print(f"   âœ… å‘ç° {results['fusion_analysis']['total_fusion_opportunities']} ä¸ªèåˆæœºä¼š")
        print(f"   âœ… é¢„è®¡å†…å­˜èŠ‚çœ: {results['fusion_analysis']['memory_saved_mb']:.2f} MB")
        
        # 2. é‡åŒ–ç­–ç•¥å¯¹æ¯”
        print("\nğŸ“Š 2. é‡åŒ–ç­–ç•¥å¯¹æ¯”...")
        results["quantization_comparison"] = self.quantization_strategy.compare_all_precisions(dummy_input)
        
        fp32_latency = results["quantization_comparison"]["fp32"]["avg_latency_ms"]
        print(f"   âœ… FP32: {fp32_latency:.2f} ms")
        
        if "fp16" in results["quantization_comparison"]:
            fp16_latency = results["quantization_comparison"]["fp16"]["avg_latency_ms"]
            fp16_speedup = results["quantization_comparison"]["fp16"].get("speedup", 1.0)
            print(f"   âœ… FP16: {fp16_latency:.2f} ms (åŠ é€Ÿ {fp16_speedup:.2f}x)")
        
        if "avg_latency_ms" in results["quantization_comparison"]["int8"]:
            int8_latency = results["quantization_comparison"]["int8"]["avg_latency_ms"]
            int8_speedup = results["quantization_comparison"]["int8"].get("speedup", 1.0)
            print(f"   âœ… INT8: {int8_latency:.2f} ms (åŠ é€Ÿ {int8_speedup:.2f}x)")
        
        # 3. æ¨ç†å»¶è¿Ÿåˆ†è§£
        print("\nğŸ“Š 3. æ¨ç†å»¶è¿Ÿåˆ†è§£...")
        results["profiling"] = self.profiler.profile_layers(dummy_input)
        results["breakdown_chart"] = self.profiler.generate_breakdown_chart_data(results["profiling"])
        print(f"   âœ… æ€»å»¶è¿Ÿ: {results['profiling']['total_time_ms']:.2f} ms")
        print(f"   âœ… åˆ†æäº† {results['profiling']['num_layers']} å±‚")
        
        # 4. ç”Ÿæˆä¼˜åŒ–å»ºè®®
        results["recommendations"] = self._generate_recommendations(results)
        
        print("\n" + "=" * 60)
        print("âœ… ä¼˜åŒ–åˆ†æå®Œæˆ")
        print("=" * 60)
        
        return results
    
    def _generate_recommendations(self, results: Dict) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®® - å¢å¼ºç‰ˆ"""
        recommendations = []
        
        # åŸºäºèåˆåˆ†æ
        fusion_count = results["fusion_analysis"]["total_fusion_opportunities"]
        if fusion_count > 0:
            recommendations.append(
                f"âœ… å»ºè®®ä½¿ç”¨ TensorRT è¿›è¡Œç®—å­èåˆï¼Œå¯èåˆ {fusion_count} ä¸ªç®—å­ç»„åˆ"
            )
            recommendations.append(
                f"   é¢„è®¡åŠ é€Ÿ: {results['fusion_analysis'].get('estimated_speedup', 1.0):.2f}x"
            )
        
        # åŸºäºé‡åŒ–å¯¹æ¯”
        if "fp16" in results["quantization_comparison"]:
            fp16_speedup = results["quantization_comparison"]["fp16"].get("speedup", 1.0)
            if fp16_speedup > 1.5:
                recommendations.append(
                    f"âœ… å»ºè®®ä½¿ç”¨ FP16 é‡åŒ–ï¼Œå¯è·å¾— {fp16_speedup:.2f}x åŠ é€Ÿ"
                )
                fp16_size = results["quantization_comparison"]["fp16"].get("model_size_mb", 0)
                recommendations.append(
                    f"   æ¨¡å‹å¤§å°å‡å°‘è‡³: {fp16_size:.1f} MB"
                )
        
        if "speedup" in results["quantization_comparison"]["int8"]:
            int8_speedup = results["quantization_comparison"]["int8"]["speedup"]
            if int8_speedup > 2.0:
                recommendations.append(
                    f"âœ… å»ºè®®ä½¿ç”¨ INT8 é‡åŒ–ï¼Œå¯è·å¾— {int8_speedup:.2f}x åŠ é€Ÿ"
                )
                int8_size = results["quantization_comparison"]["int8"].get("model_size_mb", 0)
                recommendations.append(
                    f"   æ¨¡å‹å¤§å°å‡å°‘è‡³: {int8_size:.1f} MB"
                )
        
        # åŸºäºå»¶è¿Ÿåˆ†è§£
        breakdown = results["breakdown_chart"]
        max_idx = np.argmax(breakdown["percentages"])
        bottleneck = breakdown["labels"][max_idx]
        bottleneck_pct = breakdown["percentages"][max_idx]
        
        if bottleneck_pct > 30:
            recommendations.append(
                f"âš ï¸ ç“¶é¢ˆåœ¨ {bottleneck} å±‚ï¼ˆå  {bottleneck_pct:.1f}%ï¼‰ï¼Œå»ºè®®ä¼˜å…ˆä¼˜åŒ–"
            )
            
            # é’ˆå¯¹æ€§å»ºè®®
            if "conv" in bottleneck.lower():
                recommendations.append(
                    "   å»ºè®®: ä½¿ç”¨æ·±åº¦å¯åˆ†ç¦»å·ç§¯æˆ– MobileNet æ¶æ„"
                )
            elif "linear" in bottleneck.lower():
                recommendations.append(
                    "   å»ºè®®: ä½¿ç”¨çŸ©é˜µåˆ†è§£æˆ–çŸ¥è¯†è’¸é¦"
                )
            elif "attention" in bottleneck.lower():
                recommendations.append(
                    "   å»ºè®®: ä½¿ç”¨çº¿æ€§æ³¨æ„åŠ›æˆ–ç¨€ç–æ³¨æ„åŠ›"
                )
        
        # æ‰¹å¤„ç†å»ºè®®
        recommendations.append(
            "ğŸ’¡ å»ºè®®ä½¿ç”¨æ‰¹å¤„ç†æ¨ç†ï¼ˆbatch_size=4-8ï¼‰ä»¥æå‡ GPU åˆ©ç”¨ç‡"
        )
        
        # å†…å­˜ä¼˜åŒ–å»ºè®®
        memory_saved = results["fusion_analysis"].get("memory_saved_mb", 0)
        if memory_saved > 10:
            recommendations.append(
                f"ğŸ’¾ é€šè¿‡ç®—å­èåˆå¯èŠ‚çœ {memory_saved:.1f} MB æ˜¾å­˜"
            )
        
        return recommendations
    
    def save_results(self, results: Dict, output_path: str = "optimization_results.json"):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        # è½¬æ¢ numpy ç±»å‹ä¸º Python åŸç”Ÿç±»å‹
        def convert_types(obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {k: convert_types(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_types(item) for item in obj]
            return obj
        
        results_serializable = convert_types(results)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results_serializable, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
